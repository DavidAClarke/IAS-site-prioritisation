Sys.sleep(10)
download.file(i,
destfile = file.path("Data", target_path, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200))
spec_sprat <- read_html(file.path("Data", target_path, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
ThreatsInfo <- Threat_info(Threats_Y$Profile,Threats_Y$scientificNames)
ThreatsInfo <- Threat_info(ThreatsPresent$Profile,ThreatsPresent$scientificNames)
ThreatsInfo <- Threat_info(ThreatsPresent$Profile,ThreatsPresent$scientificNames, target_path = "websites")
Threat_info <- function(urls, species, target_path) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
download.file(i,
destfile = file.path("Data", target_path, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200))
spec_sprat <- read_html(file.path("Data", target_path, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
library(tidyverse)
library(readxl)
library(rvest)
library(purrr)
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200))
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
SNES <- read_csv(file.path("Data", "AUS_ThreatenedSpeciesList_04092020.csv"))
SNES_min <- SNES %>%
dplyr::select(`Scientific Name`, `Threatened status`, `Class` ,`Profile`) %>% #add URLs column
rename(scientificName = "Scientific Name") %>%
rename(threatStatus = "Threatened status") %>%
rename(class = "Class") %>%
mutate(threatStatus = as.factor(threatStatus)) %>%
mutate(class = as.factor(class)) %>%
filter(threatStatus != "Extinct" & threatStatus != "Extinct in the wild")
rm(SNES)
ThreatsInfo_200 <- Threat_info(SNES_min[1:200,]$Profile,SNES_min[1:200,]$scientificNames, target_path = "websites_200")
write.csv(ThreatsInfo_200, file = file.path("Data", "ThreatsInfo_200.csv"))
ThreatsInfo_200 <- Threat_info(SNES_min[1:200,]$Profile,SNES_min[1:200,]$scientificNames, target_folder = "websites_200")
write.csv(ThreatsInfo_200, file = file.path("Data", "ThreatsInfo_200.csv"))
ThreatsInfo_200 <- Threat_info(SNES_min[1:200,]$Profile,SNES_min[1:200,]$scientificName, target_folder = "websites_200")
write.csv(ThreatsInfo_200, file = file.path("Data", "ThreatsInfo_200.csv"))
#Keywords
IAS_keywords <- c("feral", "exotic", "invasive", "non-native")
species_keywords <- sapply(IAS_keywords[1:length(IAS_keywords)], function(i) {
if_else(str_detect(ThreatsInfo$ThreatInfo_200, i) == T, 1, 0)
})
species_keywords <- sapply(IAS_keywords[1:length(IAS_keywords)], function(i) {
if_else(str_detect(ThreatsInfo_200$ThreatInfo, i) == T, 1, 0)
})
IAS_threats <- if_else(rowSums(species_keywords) > 0, 1, 0)
species_keywords <- sapply(IAS_keywords[1:length(IAS_keywords)], function(i) {
if_else(str_detect(ThreatsInfo_200$ThreatInfo, i) == T, 1, 0)
})
IAS_threats <- if_else(rowSums(species_keywords) > 0, 1, 0)
ThreatsInfo <- ThreatsInfo_200 %>%
mutate(IAS_threats = IAS_threats) %>%
dplyr::select(scientificNames, IAS_threats)
head(ThreatsInfo)
sum(ThreatsInfo$IAS_threats)
ThreatsInfo_400 <- Threat_info(SNES_min[201:400,]$Profile,SNES_min[201:400,]$scientificName, target_folder = "websites_400")
write.csv(ThreatsInfo_400, file = file.path("Data", "ThreatsInfo_400.csv"))
ThreatsInfo_400 <- Threat_info(SNES_min[201:400,]$Profile,SNES_min[201:400,]$scientificName, target_folder = "websites_400")
write.csv(ThreatsInfo_400, file = file.path("Data", "ThreatsInfo_400.csv"))
ThreatsInfo_600 <- Threat_info(SNES_min[401:600,]$Profile,SNES_min[401:600,]$scientificName, target_folder = "websites_600")
write.csv(ThreatsInfo_600, file = file.path("Data", "ThreatsInfo_600.csv"))
url <- "http://www.environment.gov.au/cgi-bin/sprat/public/publicspecies.pl?taxon_id=17085"
browseURL(url, browser = "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe")
getOption("browser")
getOption("timeout")
chrome <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200),
browser = chrome)
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
ThreatsInfo_600 <- Threat_info(SNES_min[401:600,]$Profile,SNES_min[401:600,]$scientificName, target_folder = "websites_600")
write.csv(ThreatsInfo_600, file = file.path("Data", "ThreatsInfo_600.csv"))
ThreatsInfo_600 <- Threat_info(SNES_min[401:600,]$Profile,SNES_min[401:600,]$scientificName, target_folder = "websites_600")
write.csv(ThreatsInfo_600, file = file.path("Data", "ThreatsInfo_600.csv"))
ThreatsInfo_800 <- Threat_info(SNES_min[601:800,]$Profile,SNES_min[601:800,]$scientificName, target_folder = "websites_800")
write.csv(ThreatsInfo_800, file = file.path("Data", "ThreatsInfo_800.csv"))
ThreatsInfo_1000 <- Threat_info(SNES_min[801:1000,]$Profile,SNES_min[801:1000,]$scientificName, target_folder = "websites_1000")
write.csv(ThreatsInfo_1000, file = file.path("Data", "ThreatsInfo_1000.csv"))
remotes::install_github("mikejohnson51/AOI") # suggested!
remotes::install_github("mikejohnson51/climateR")
install.packages("RNetCDF")
remotes::install_github("mikejohnson51/climateR")
ThreatsInfo_1800 <- Threat_info(SNES_min[1001:1799,]$Profile,SNES_min[1001:1799,]$scientificName, target_folder = "websites_1800")
write.csv(ThreatsInfo_1800, file = file.path("Data", "ThreatsInfo_1800.csv"))
ThreatsInfo_1200 <- Threat_info(SNES_min[1001:1200,]$Profile,SNES_min[1001:1200,]$scientificName, target_folder = "websites_1200")
write.csv(ThreatsInfo_1200, file = file.path("Data", "ThreatsInfo_1200.csv"))
?withTimeout
??withTimeout
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
#It helps to have chrome as the default browser
withTimeout(download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200)
),timeout = 200)
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
ThreatsInfo_1200 <- Threat_info(SNES_min[1001:1200,]$Profile,SNES_min[1001:1200,]$scientificName, target_folder = "websites_1200")
write.csv(ThreatsInfo_1200, file = file.path("Data", "ThreatsInfo_1200.csv"))
ThreatsInfo_1200 <- Threat_info(SNES_min[1001:1200,]$Profile,SNES_min[1001:1200,]$scientificName, target_folder = "websites_1200")
write.csv(ThreatsInfo_1200, file = file.path("Data", "ThreatsInfo_1200.csv"))
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
#It helps to have chrome as the default browser
R.utils::withTimeout(download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200)
),timeout = 200)
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
install.packages("bossMaps")
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
#It helps to have chrome as the default browser
R.utils::withTimeout(download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200)
),timeout = 200)
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
ThreatsInfo_1200 <- Threat_info(SNES_min[1001:1200,]$Profile,SNES_min[1001:1200,]$scientificName, target_folder = "websites_1200")
write.csv(ThreatsInfo_1200, file = file.path("Data", "ThreatsInfo_1200.csv"))
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
#It helps to have chrome as the default browser
options(timeout = 200)
download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200)
)
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
ThreatsInfo_1200 <- Threat_info(SNES_min[1001:1200,]$Profile,SNES_min[1001:1200,]$scientificName, target_folder = "websites_1200")
write.csv(ThreatsInfo_1200, file = file.path("Data", "ThreatsInfo_1200.csv"))
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
#It helps to have chrome as the default browser
options(timeout = 300)
download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200)
)
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
ThreatsInfo_1200 <- Threat_info(SNES_min[1001:1200,]$Profile,SNES_min[1001:1200,]$scientificName, target_folder = "websites_1200")
write.csv(ThreatsInfo_1200, file = file.path("Data", "ThreatsInfo_1200.csv"))
library(tidyverse)
library(readxl)
library(rvest)
library(purrr)
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
#It helps to have chrome as the default browser
options(timeout = 300)
download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(timeout = 200)
)
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
SNES <- read_csv(file.path("Data", "AUS_ThreatenedSpeciesList_04092020.csv"))
SNES_min <- SNES %>%
dplyr::select(`Scientific Name`, `Threatened status`, `Class` ,`Profile`) %>% #add URLs column
rename(scientificName = "Scientific Name") %>%
rename(threatStatus = "Threatened status") %>%
rename(class = "Class") %>%
mutate(threatStatus = as.factor(threatStatus)) %>%
mutate(class = as.factor(class)) %>%
filter(threatStatus != "Extinct" & threatStatus != "Extinct in the wild")
rm(SNES)
ThreatsInfo_1400 <- Threat_info(SNES_min[1201:1400,]$Profile,SNES_min[1201:1400,]$scientificName, target_folder = "websites_1400")
write.csv(ThreatsInfo_1400, file = file.path("Data", "ThreatsInfo_1400.csv"))
ThreatsInfo_1600 <- Threat_info(SNES_min[1401:1600,]$Profile,SNES_min[1401:1600,]$scientificName, target_folder = "websites_1600")
write.csv(ThreatsInfo_1600, file = file.path("Data", "ThreatsInfo_1600.csv"))
Threat_info <- function(urls, species, target_folder) {
scientificNames <- as.vector(species)
ThreatInfo <- vector(mode = "character", length = length(urls))
dir.create(file.path("Data", target_folder))
res.out <- lapply(urls[1:length(urls)], function(i = urls) {
print(i)
Sys.sleep(10)
#It helps to have chrome as the default browser
options(timeout = 300)
download.file(i,
destfile = file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")),
quiet = T,
extra = options(download.file.method = "libcurl")
)
spec_sprat <- read_html(file.path("Data", target_folder, paste0(which(urls == i, arr.ind = T), "_scrapedpage.html")))
ThreatInfo <- spec_sprat %>%
html_element("#bodyContent") %>%
html_text()
})
res.out <- do.call(rbind,res.out)
res.out <- data.frame(scientificNames = scientificNames,
ThreatInfo = res.out)
return(as_tibble(res.out))
}
ThreatsInfo_1600 <- Threat_info(SNES_min[1401:1600,]$Profile,SNES_min[1401:1600,]$scientificName, target_folder = "websites_1600")
write.csv(ThreatsInfo_1600, file = file.path("Data", "ThreatsInfo_1600.csv"))
old_files <- list.files(file.path("SpatialData", "Input_zonation", "PAs"), pattern = "*.tif", full.names = T)
head(old_files)
?grepl
new_files <- grepl(" ", old_files)
head(new_files)
old_files <- list.files(file.path("SpatialData", "Input_zonation", "PAs"), pattern = "*.tif", full.names = F)
new_files <- grepl(" ", old_files)
new_files
new_files[TRUE]
new_files[[TRUE]]
new_files <- new_files == T
new_files
new_files <- new_files[new_files == T]
new_files
length(new_files)
8031-297
old_files[grepl(" ", old_files)]
old_files <- list.files(file.path("SpatialData", "Input_zonation", "PAs"), pattern = "*.tif", full.names = T)
to_remove <- old_files[grepl(" ", old_files)]
file.remove(to_remove)
ThreatsInfo_1800 <- Threat_info(SNES_min[1601:1799,]$Profile,SNES_min[1601:1799,]$scientificName, target_folder = "websites_1800")
write.csv(ThreatsInfo_1800, file = file.path("Data", "ThreatsInfo_1800.csv"))
ThreatsInfo_1800 <- Threat_info(SNES_min[1601:1799,]$Profile,SNES_min[1601:1799,]$scientificName, target_folder = "websites_1800")
write.csv(ThreatsInfo_1800, file = file.path("Data", "ThreatsInfo_1800.csv"))
ThreatsInfo_200 <- read.csv(file.path("Data", "ThreatsInfo_200.csv"))
ThreatsInfo_400 <- read.csv(file.path("Data", "ThreatsInfo_400.csv"))
ThreatsInfo_600 <- read.csv(file.path("Data", "ThreatsInfo_600.csv"))
ThreatsInfo_800 <- read.csv(file.path("Data", "ThreatsInfo_800.csv"))
ThreatsInfo_1000 <- read.csv(file.path("Data", "ThreatsInfo_1000.csv"))
ThreatsInfo_1200 <- read.csv(file.path("Data", "ThreatsInfo_1200.csv"))
ThreatsInfo_1400 <- read.csv(file.path("Data", "ThreatsInfo_1400.csv"))
ThreatsInfo_1600 <- read.csv(file.path("Data", "ThreatsInfo_1600.csv"))
ThreatsInfo_1800 <- read.csv(file.path("Data", "ThreatsInfo_1800.csv"))
ThreatsInfo <- rbind(ThreatsInfo_200, ThreatsInfo_400, ThreatsInfo_600,
ThreatsInfo_800, ThreatsInfo_1000, ThreatsInfo_1200,
ThreatsInfo_1400, ThreatsInfo_1600, ThreatsInfo_1800)
write.csv(ThreatsInfo, file = file.path("Data", "ThreatsInfo.csv"))
IAS_keywords <- c("feral", "exotic", "invasive", "non-native")
# Determining if SPRAT information contains information on IAS
# This technically works.
species_keywords <- sapply(IAS_keywords[1:length(IAS_keywords)], function(i) {
if_else(str_detect(ThreatsInfo$ThreatInfo, i) == T, 1, 0)
})
IAS_threats <- if_else(rowSums(species_keywords) > 0, 1, 0)
ThreatsInfo <- ThreatsInfo %>%
mutate(IAS_threats = IAS_threats) %>%
dplyr::select(scientificNames, IAS_threats)
head(ThreatsInfo)
sum(ThreatsInfo$IAS_threats)
# Join ThreatsInfo to SNES_min to get "class" information for the species threatened by IAS
SNES_min_test <- left_join(SNES_min, ThreatsInfo, by = "scientificName")
ThreatsInfo <- ThreatsInfo %>% rename(scientificName = "scientificNames")
head(ThreatsInfo)
# Join ThreatsInfo to SNES_min to get "class" information for the species threatened by IAS
SNES_min_test <- left_join(SNES_min, ThreatsInfo, by = "scientificName")
head(SNES_min_test)
# Join ThreatsInfo to SNES_min to get "class" information for the species threatened by IAS
SNES_min <- left_join(SNES_min, ThreatsInfo, by = "scientificName")
library(sf)
SNES_shp <- st_read("SpatialData/Vector/snes_public_grids_08Aug2019_shapefile/snes_species_combined.shp")
SNES_shp_min <- SNES_shp %>%
drop_na(threatened) %>%
filter(pres_rank == "2" & threatened != "Extinct in the wild") %>% # 1 = species may occur; 2 = species likely occur
select(sci_name, threatened, tax_group, tax_class) %>%
rename(scientificName = "sci_name") %>%
rename(threatStatus = "threatened") %>%
rename(class = "tax_class") %>%
rename(group = "tax_group") %>%
mutate(threatStatus = factor(threatStatus,
levels = c("Conservation Dependent","Vulnerable",
"Endangered", "Critically Endangered"),
labels = c("CD", "VU", "EN", "CR"))) %>%
mutate(class = as.factor(class)) %>%
mutate(group = as.factor(group))
head(SNES_shp_min)
length(unique(SNES_shp_min$scientificName))
rm(SNES_shp)
rm(SNES_min_test)
rm(ThreatsInfo_200,ThreatsInfo_400,ThreatsInfo_600,ThreatsInfo_800,ThreatsInfo_1000,ThreatsInfo_1200,ThreatsInfo_1400,ThreatsInfo_1600,ThreatsInfo_1800)
# Cleaning and pre-processing of input data #
# Australian coast
source("Scripts/Coast.R")
#Combining spatial and ancillary information
SNES_shp <- st_crop(SNES_shp_min, Aus_Coast) #bringing extent down before projection
#Combining spatial and ancillary information
SNES_shp <- merge(ThreatsInfo, SNES_shp, by = "scientificName")
#Combining spatial and ancillary information
SNES_shp <- merge(ThreatsInfo, SNES_shp_min, by = "scientificName")
SNES_shp <- st_as_sf(SNES_shp) #now ready for further spatial analysis
head(SNES_shp)
head(SNES_shp)
length(unique(SNES_shp$IAS_threats))
SNES_IAS_shp <- SNES_shp %>% filter(IAS_threats == 1)
length(unique(SNES_IAS_shp$scientificName))
library(rdrop2)
drop_acc()
has_element(drop_acc())
?has_element
is_empty(drop_acc())
if(is_empty(drop_acc()) == F) {print("Hooray")}
#~# Clears the environment
rm(list=ls())
c
c
Q
library(rdrop2)
?drop_download
drop_download
?sprintf
?message
message("sup")
drop_download
message(sprintf("Downloaded %s to %s: %s on disk")
message(sprintf("Downloaded %s to %s: %s on disk"))
message(sprintf("Downloaded %s to %s: %s on disk"), path, local_path)
message(sprintf("Downloaded %s to %s: %s on disk", path, local_path, format(size, units = "auto")))
testit <- function() {
message("testing package startup messages")
packageStartupMessage("initializing ...", appendLF = FALSE)
Sys.sleep(1)
packageStartupMessage(" done")
}
testit()
testit <- function() {
message("testing package startup messages")
packageStartupMessage("initializing ...", appendLF = FALSE)
Sys.sleep(5)
packageStartupMessage(" done")
}
testit()
# Load required libraries
library(tidyverse)
library(sf)
SNES_shp <- st_read("SpatialData/Vector/snes_public_grids_08Aug2019_shapefile/snes_species_combined.shp")
SNES_shp_min <- SNES_shp %>%
drop_na(threatened) %>%
filter(pres_rank == "2" & threatened != "Extinct in the wild") %>% # 1 = species may occur; 2 = species likely occur
dplyr::select(sci_name, threatened, tax_group, tax_class) %>%
rename(scientificName = "sci_name") %>%
rename(threatStatus = "threatened") %>%
rename(class = "tax_class") %>%
rename(group = "tax_group") %>%
mutate(threatStatus = factor(threatStatus,
levels = c("Conservation Dependent","Vulnerable",
"Endangered", "Critically Endangered"),
labels = c("CD", "VU", "EN", "CR"))) %>%
mutate(class = as.factor(class)) %>%
mutate(group = as.factor(group))
rm(SNES_shp)
head(SNES_shp_min)
SNES_shp_min[["scientificName"]]
head(SNES_shp_min[["scientificName"]])
snes <- head(SNES_shp_min[["scientificName"]])
SNES_shp_min[["scientificName"]] <- gsub(" |/", "_", SNES_shp_min[["scientificName"]])
heasd(SNES_shp_min)
head(SNES_shp_min)
library(terra)
here()
library(here)
here()
ias <- rast("C:/Users/david/Documents/PhD/Chapter_3/SpatialData/ias_richness_sp.tif")
caz <- rast("C:/Users/david/Documents/PhD/Chapter_3/SpatialData/CAZ_wgt_var_ras.tif")
plot(caz)
plot(ias)
ias_st <- ias %>% stars::st_as_stars() %>% sf::st_as_sf()
library(tidyverse)
ias_st <- ias %>% stars::st_as_stars() %>% sf::st_as_sf()
caz_st <- caz %>% stars::st_as_stars() %>% sf::st_as_sf()
